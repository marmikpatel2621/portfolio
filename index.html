
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Marmik Patel | Data Engineer & Solution Architect</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <style>
    * { box-sizing: border-box; }
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9fafb;
      color: #1f2937;
      scroll-behavior: smooth;
    }
    header {
      background: #1f2937;
      color: #fff;
      padding: 2rem 1rem;
      text-align: center;
    }
    header img {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      object-fit: cover;
      margin-bottom: 1rem;
    }
    header h1 {
      font-size: 2.5rem;
      margin: 0.5rem 0;
    }
    header p {
      font-size: 1.1rem;
      margin: 0;
    }
    nav {
      position: sticky;
      top: 0;
      background: #1f2937;
      z-index: 1000;
      padding: 0.5rem 0;
      text-align: center;
    }
    nav a {
      color: #9ca3af;
      margin: 0 1rem;
      text-decoration: none;
      font-weight: 500;
    }
    nav a:hover, nav a.active {
      color: #fff;
      font-weight: bold;
      text-decoration: underline;
    }
    .container {
      max-width: 1100px;
      margin: 2rem auto;
      padding: 0 1.5rem;
    }
    section {
      margin-bottom: 3rem;
    }
    h2 {
      font-size: 1.8rem;
      margin-bottom: 1rem;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 0.5rem;
    }
    .project, .experience-block {
      background: #fff;
      padding: 1.5rem;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.04);
      margin-bottom: 1.5rem;
    }
    .project h3, .experience-block h3 {
      font-size: 1.25rem;
      margin-bottom: 0.5rem;
      color: #111827;
    }
    ul { padding-left: 1.2rem; }
    li { margin-bottom: 0.4rem; }
    .skills span {
      display: inline-block;
      background: #e5e7eb;
      color: #374151;
      margin: 0.3rem;
      padding: 0.4rem 0.8rem;
      border-radius: 999px;
      font-size: 0.9rem;
    }
    .skills span:hover { background: #d1d5db; }
    .download a {
      display: inline-block;
      margin-top: 1rem;
      background: #2563eb;
      color: #fff;
      padding: 0.6rem 1.2rem;
      border-radius: 6px;
      text-decoration: none;
    }
    .download a:hover { background: #1d4ed8; }
    footer {
      background: #1f2937;
      color: #9ca3af;
      text-align: center;
      padding: 1rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Marmik Patel</h1>
    <p>Senior Data Engineer & Solution Architect</p>
    <nav>
      <a href="#about">About</a>
      <a href="#experience">Experience</a>
      <a href="#projects">Projects</a>
      <a href="#skills">Skills</a>
      <a href="#resume">Resume</a>
    </nav>
  </header>
  <div class="container">
    <section id="about">
      <h2>About</h2>
      <p>I‚Äôm a results-driven Senior Data Engineer & Solution Architect with 8+ years of experience delivering scalable, high-performance data platforms for Fortune 500 enterprises across industries like telecom, life sciences, and finance.</p>
      <ul>
        <li>üì¶ Architected data platforms ingesting 1.8B+ records/hour with Spark and Snowflake</li>
        <li>‚öôÔ∏è Delivered 1500+ production-grade ETL jobs/hour with sub-5-min SLAs</li>
        <li>üìâ Led migration initiatives cutting query latency by 80% and infrastructure cost by 30%</li>
        <li>üí¨ Cross-functional leader skilled in mentoring engineers and collaborating with product stakeholders</li>
      </ul>
    </section>

    <section id="experience">
      <h2>Work Experience</h2>
      <div class="experience-block">
        <h3>Squadron Data Inc., Overland Park, KS ‚Äî Senior Solution Architect</h3>
        <p><strong>Jan 2024 ‚Äì Present</strong></p>
        <ul>
          <li>Lead architect for cloud-scale data-platform modernizations, delivering a Snowflake + Snowpark migration factory (1 TB/hr, 700 ETL jobs/hr) and a Spark 3 upgrade that cut cluster cost 20 %.</li>
          <li>Built out secure Azure‚ÄìSnowflake foundation (Private Link, RBAC, multi-cluster warehouses) and rolled out CI/CD + monitoring that trims release times and holds 99.8 % SLA.</li>
          <li>Mentor and code-review lead for a five-engineer team; introduced CI/CD pipelines that trim release cycles by 40 %</li>
        </ul>
      </div>
      <div class="experience-block">
        <h3>Squadron Data Inc., Overland Park, KS ‚Äî Big Data Hadoop Developer</h3>
        <p><strong>Jan 2018 ‚Äì Dec 2023</strong></p>
        <ul>
          <li>Designed the Hive Query Stats Spark application (Scala) to surface YARN metrics, informing capacity planning and saving clients >$200 K/yr</li>
          <li>Created the Squadron Platform Migration Tool, automating Teradata/Netezza‚Äëto‚ÄëHadoop/Snowflake migrations</li>
          <li>Led HDP ‚Üí CDP in-place upgrades for seven enterprise clusters (150 K tables), implementing Kerberos, Ranger, TLS/SSL, and ACID adoption playbooks</li>
        </ul>
      </div>
    </section>

    <section id="projects">
      <h2>Projects</h2>
      <div class="project">
        <h3>Snowflake Migration Factory & Real-Time Analytics Enablement</h3>
        <ul>
          <li>Migrated <strong>700+ databases / 4500+ tables</strong> from Hive to Snowflake with Snowpark-driven automated schema-transformation logic.</li>
          <li>Designed Azure‚ÄìSnowflake network topology: deployed Private Endpoints (PE / PEP), routed traffic through Azure Private Link and DNS forwarding rules to keep all data-in-flight off the public Internet.</li>
          <li>Provisioned multi-cluster warehouses (XS ‚Üí L) with auto-suspend / auto-resume and resource monitors; drove 20 % credit savings while meeting burst-load SLAs.</li>
          <li>Implemented role-based access control (RBAC): created custom roles for Consumer Teams vs Platform Ops, integrated Azure AD SSO, and built user-onboarding scripts with SnowSQL + Github Actions</li>
          <li>Configured <strong>Snowflake Tasks & Streams + Snowpipe</strong> for near-real-time ingestion; orchestrated cross-database dependencies via Airflow DAGs.</li>
          <li>Tuned file formats & micro-partition clustering enabled the sustainment of over <strong>1500 ETL jobs/hour</strong>, while maintaining query latency below a second for more than 40 downstream teams.</li>
          <li><strong>Impact:</strong> Pipeline execution is 3√ó faster, achieving 99.8% SLA compliance, and audited with zero egress to public endpoints for compliance.</li>
        </ul>
      </div>
      <div class="project">
        <h3>Spark2 to Spark3 Upgrade & Cluster Optimization</h3>
        <ul>
          <li><strong>Refactored 400+ Spark 2 jobs to Spark 3</strong>, exploiting Adaptive Query Execution (AQE), dynamic partition pruning, and skew-aware shuffle.</li>
          <li><strong>Fine-tuned cluster configs:</strong> adjusted executor/core ratios, shuffle partitions, GC flags, and I/O buffer sizes; trimmed median job memory by 30 %.</li>
          <li><strong>Curated dependency set</strong> (Spark, Hadoop, Hive, Scala libs) and removed shaded JAR conflicts, shrinking container start-up time by 20 %.</li>
          <li>Implemented <strong>bucketed tables + predicate push-down</strong>, optimized broadcast vs. shuffle joins, and rewrote wide joins with Bloom filters.</li>
          <li>Slashed small-file count 80% via compaction strategy and dynamic repartitioning; end-to-end runtime 1.5 ‚Äì 2√ó faster on daily workload.</li>
          <li>Authored a <strong>Spark 3 performance playbook</strong> with config templates and code patterns adopted by 6 peer teams.</li>
          <li><strong>Impact:</strong> 20% reduction in Yarn-cluster CPU / memory cost while sustaining tighter SLAs for downstream analytics.</li>
        </ul>
      </div>
      <div class="project">
        <h3>Enterprise Hadoop Modernization (HDP ‚Üí CDP)</h3>
        <ul>
          <li><strong>Assessed 2,000-node HDP estates:</strong> profiled services, Hive metastore health (150K+ tables) and security posture to choose in-place upgrade vs. side-by-side migration per cluster.</li>
          <li>Created <strong>‚Äúcode-delta‚Äù playbooks</strong> ‚Äî flagged Spark/Hive syntax breaks, Sqoop/OOZIE job tweaks, and storage-path changes‚Äîso application teams could refactor before cut-over.</li>
          <li>Authored and co-owned a <strong>modular runbook framework</strong>; generated environment-specific runbooks and automated validation scripts for DEV ‚Üí UAT ‚Üí PROD waves.</li>
          <li>Implemented Ranger policies, Kerberos, Knox gateway, TLS/SSL and enabled Hive/Spark ACID tables; handed over post-go-live hardening checklist to platform ops.</li>
        </ul>
      </div>
      <div class="project">
        <h3>Telecom ETL Optimization & Small File Reduction</h3>
        <ul>
          <li><strong>Audited legacy Spark & Hive codebase</strong> to surface inefficient shuffle patterns, hard-coded partitions, and file-layout anti-patterns. </li>
          <li><strong>Redesigned Hive partition scheme + file-format strategy</strong> (multiple buckets, larger ORC stripes) for Tez; eliminated small-file explosion.</li>
          <li><strong>Re-architected Spark ingest framework</strong> to sustain 1‚Äì2 B rows/hour with checkpointing and dynamic repartitioning.</li>
          <li><strong>Result:</strong> 10√ó higher ingest throughput and Hive query latency cut from 6 min to around 1 min.</li>

        </ul>
      </div>
      <div class="project">
        <h3>Teradata to Hadoop Migration Framework</h3>
        <ul>
          <li><strong>Parsed historical Teradata query logs</strong> with Spark to profile access patterns and recommend optimal HDFS layout, clustering keys, and partition strategy.</li>
          <li>Generated Sqoop-ready table manifests and an <strong>auto-builder that emits parallel Sqoop scripts</strong> for 600+ databases / 3,500+ tables.</li>
          <li>Produced companion module that scaffolds <strong>Spark ETL templates</strong> (read, transform, write to Hive) for each migrated table‚Äîaccelerating developer onboarding and ensuring performance best practices.</li>

        </ul>
      </div>
    </section>

    <section id="skills">
      <h2>Skills & Tools</h2>
      <div class="skills">
        <span>Hadoop</span><span>Spark 2/3</span><span>Snowflake</span><span>Hive</span><span>Kafka</span><span>Livy</span>
        <span>Snowpark</span><span>AWS</span><span>Azure</span><span>Docker</span><span>Python</span><span>Scala</span>
        <span>Java</span><span>SQL</span><span>Bash</span><span>Airflow</span><span>Oozie</span><span>DataStage</span>
        <span>Power BI</span><span>Tableau</span><span>Maven</span><span>SBT</span><span>Git</span><span>CI/CD</span>
      </div>
    </section>

    <section id="resume">
      <h2>Resume & Contact</h2>
      <p><strong>Email:</strong> <a href="mailto:marmikpatel262@gmail.com">marmikpatel262@gmail.com</a></p>
      <p><strong>Location:</strong> Spring Hill, KS</p>
      <p><strong>Certification:</strong> SnowPro Core Certified (Snowflake)</p>
      <div class="download">
        <a href="Marmik_Patel_Resume.pdf" download>üìÑ Download Resume (PDF)</a>
      </div>
    </section>
  </div>
  <footer>
    <p>&copy; 2025 Marmik Patel</p>
  </footer>
</body>
</html>
